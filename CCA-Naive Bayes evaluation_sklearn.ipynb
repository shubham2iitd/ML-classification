{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import nltk\n",
      "import numpy as np\n",
      "from datetime import datetime\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pd.set_option('display.height', 300)\n",
      "pd.set_option('display.max_rows', 500)\n",
      "pd.set_option('display.max_columns', 50)\n",
      "pd.set_option('display.width', 200)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "height has been deprecated.\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import collections\n",
      "import nltk.metrics"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "import string\n",
      "import os\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from nltk.stem.porter import PorterStemmer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "token_dict = {}\n",
      "stemmer = PorterStemmer()\n",
      "\n",
      "def stem_tokens(tokens, stemmer):\n",
      "    stemmed = []\n",
      "    for item in tokens:\n",
      "        stemmed.append(stemmer.stem(item))\n",
      "    return stemmed\n",
      "\n",
      "def tokenize(text):\n",
      "    tokens = nltk.word_tokenize(text)\n",
      "#    stems = stem_tokens(tokens, stemmer)\n",
      "#    return stems\n",
      "    return tokens"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import unicodedata\n",
      "\n",
      "def unaccent( text ):\n",
      "    s = unicodedata.normalize( 'NFKD', text )\n",
      "    s = ''.join( c for c in s if ord( c ) < 127 )\n",
      "    return s"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def document_features(document):\n",
      "    document_tokens = nltk.word_tokenize(document)\n",
      "    document_words = set(document_tokens) \n",
      "    features = {}\n",
      "    for word in word_features:\n",
      "        features['contains(%s)' % word] = (str(word) in document_words)\n",
      "    return features"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import division\n",
      "import random\n",
      "from random import sample\n",
      "import operator"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "commodity = \"Hard Drive\"\n",
      "column_tr = \"CIDAR_PNP_FOLLOWED\"\n",
      "column_tst = \"CIDAR PNP FOLLOWED\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "DF = pd.read_csv(\"S:\\S drive data\\Projects\\CCA-Dell\\Ouput from Genie\\ABU_Ashu_28_7.txt\",sep = \"\\t\")\n",
      "DF[\"free_text\"] = DF.apply(lambda x: str(x[\"COMMENTS\"]) +\" \"+str(x[\"TEXT\"])+\" \"+str(x[\"Agent_Description\"])+\" \"+str(x[\"Notes\"]),1)\n",
      "DF.drop([\"COMMENTS\",\"TEXT\",\"Agent_Description\",\"Notes\",\"ISDIAGPOSSIBLE\"],1)\n",
      "DF[\"free_text\"] = DF.free_text.str.lower()\n",
      "DF = DF[DF[\"COMMODITYDESC\"] == commodity]\n",
      "DF = DF[DF[\"rand_Sel\"] == 1]\n",
      "DF.index = range(len(DF))\n",
      "print len(DF)\n",
      "rows1 = random.sample(DF.index, 1000)\n",
      "df1_tr = DF.ix[rows1]\n",
      "df1_tr.index = range(len(df1_tr))\n",
      "df1_tr[\"ISDIAGPOSSIBLE\"] = df1_tr[column_tr]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "15488\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "replace_punctuation = string.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
      "for i in range(len(df1_tr)):\n",
      "    df1_tr[\"free_text\"][i] = df1_tr[\"free_text\"][i].translate(replace_punctuation)\n",
      "    df1_tr[\"free_text\"][i] = df1_tr[\"free_text\"][i].decode('utf-8','ignore')\n",
      "    df1_tr[\"free_text\"][i] = unaccent(df1_tr[\"free_text\"][i])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pandas import DataFrame\n",
      "data_frame1 = DataFrame({'text': [], 'class': []})\n",
      "\n",
      "for i in range(len(df1_tr)):\n",
      "    data_frame1 = data_frame1.append(DataFrame({'text': [df1_tr[\"free_text\"][i]], 'class': [df1_tr[\"ISDIAGPOSSIBLE\"][i]]}))\n",
      "data_frame1.index = range(len(data_frame1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "DF1 = pd.read_csv(\"S:\\S drive data\\Projects\\CCA-Dell\\Test data sets\\ABU_test dataset.txt\",sep = \"\\t\")\n",
      "DF1[\"free_text\"] = DF1.apply(lambda x: str(x[\"COMMENTS\"]) +\" \"+str(x[\"TEXT\"])+\" \"+str(x[\"AGENT_DESCRIPTION\"])+\" \"+str(x[\"NOTES\"]),1)\n",
      "DF1.drop([\"COMMENTS\",\"TEXT\",\"AGENT_DESCRIPTION\",\"NOTES\",\"ISDIAGPOSSIBLE\"],1)\n",
      "DF1[\"free_text\"] = DF1.free_text.str.lower()\n",
      "DF1 = DF1[DF1[\"COMMODITYDESC\"] == commodity]\n",
      "DF1.index = range(len(DF1))\n",
      "print len(DF1)\n",
      "rows2 = random.sample(DF1.index, 5000)\n",
      "df1_tst = DF1.ix[rows2]\n",
      "df1_tst.index = range(len(df1_tst))\n",
      "df1_tst[\"ISDIAGPOSSIBLE\"] = df1_tst[column_tst]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "9183\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "C:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py:1159: DtypeWarning: Columns (48) have mixed types. Specify dtype option on import or set low_memory=False.\n",
        "  data = self._reader.read(nrows)\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "replace_punctuation = string.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
      "for i in range(len(df1_tst)):\n",
      "    df1_tst[\"free_text\"][i] = df1_tst[\"free_text\"][i].translate(replace_punctuation)\n",
      "    df1_tst[\"free_text\"][i] = df1_tst[\"free_text\"][i].decode('utf-8','ignore')\n",
      "    df1_tst[\"free_text\"][i] = unaccent(df1_tst[\"free_text\"][i])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_frame1 = pd.DataFrame({'text': [], 'class': []})\n",
      "\n",
      "for i in range(len(df1_tr)):\n",
      "    data_frame1 = data_frame1.append(pd.DataFrame({'text': [df1_tr[\"free_text\"][i]], 'class': [df1_tr[\"ISDIAGPOSSIBLE\"][i]]}))\n",
      "data_frame1.index = range(len(data_frame1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_frame2 = pd.DataFrame({'text': [], 'class': []})\n",
      "\n",
      "for i in range(len(df1_tst)):\n",
      "    data_frame2 = data_frame2.append(pd.DataFrame({'text': [df1_tst[\"free_text\"][i]], 'class': [df1_tst[\"ISDIAGPOSSIBLE\"][i]]}))\n",
      "data_frame2.index = range(len(data_frame2))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "count_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
      "counts = count_vectorizer.fit_transform(numpy.asarray(data_frame1['text']))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.naive_bayes import MultinomialNB\n",
      "\n",
      "classifier = MultinomialNB()\n",
      "targets = numpy.asarray(data_frame1['class'], dtype=\"|S6\")\n",
      "\n",
      "classifier.fit(counts, targets)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 34,
       "text": [
        "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "examples = [data_frame2[\"text\"][i] for i in range(5)]\n",
      "example_counts = count_vectorizer.transform(examples)\n",
      "predictions = classifier.predict(example_counts)\n",
      "predictions.tolist()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "UnicodeDecodeError",
       "evalue": "'utf8' codec can't decode byte 0x96 in position 194: invalid start byte",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-35-74bffe39781f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mexamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdata_frame2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mexample_counts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcount_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexample_counts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.pyc\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m    865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    866\u001b[0m         \u001b[1;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 867\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    868\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    869\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.pyc\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    746\u001b[0m         \u001b[0mindptr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    747\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 748\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    749\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m                     \u001b[0mj_indices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[1;32m--> 234\u001b[1;33m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.pyc\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode_error\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Anaconda\\lib\\encodings\\utf_8.pyc\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(input, errors)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'strict'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutf_8_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalEncoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf8' codec can't decode byte 0x96 in position 194: invalid start byte"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "type(classifier)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 72,
       "text": [
        "sklearn.naive_bayes.MultinomialNB"
       ]
      }
     ],
     "prompt_number": 72
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.feature_extraction.text import TfidfTransformer\n",
      "\n",
      "pipeline = Pipeline([\n",
      "  ('vectorizer',  CountVectorizer(ngram_range=(1, 2))),\n",
      "#  ('tfidf_transformer',  TfidfTransformer()),\n",
      "  ('classifier',  MultinomialNB()) ])\n",
      "\n",
      "pipeline.fit(numpy.asarray(data_frame1['text']), numpy.asarray(data_frame1['class'], dtype=\"|S6\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 89,
       "text": [
        "Pipeline(steps=[('vectorizer', CountVectorizer(analyzer=u'word', binary=False, charset=None,\n",
        "        charset_error=None, decode_error=u'strict',\n",
        "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
        "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
        "        ngram_range=(1, 2)...None, vocabulary=None)), ('classifier', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
       ]
      }
     ],
     "prompt_number": 89
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "type(pipeline)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 94,
       "text": [
        "sklearn.pipeline.Pipeline"
       ]
      }
     ],
     "prompt_number": 94
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import KFold\n",
      "\n",
      "k_fold = KFold(n=len(data_frame1), n_folds=6, indices=False)\n",
      "scores = []\n",
      "for train_indices, test_indices in k_fold:\n",
      "  train_text = numpy.asarray(data_frame1[train_indices]['text'])\n",
      "  train_y    = numpy.asarray(data_frame1[train_indices]['class'], dtype=\"|S6\")\n",
      "\n",
      "  test_text  = numpy.asarray(data_frame1[test_indices]['text'])\n",
      "  test_y     = numpy.asarray(data_frame1[test_indices]['class'], dtype=\"|S6\")\n",
      "\n",
      "  pipeline.fit(train_text, train_y)\n",
      "  score = pipeline.score(test_text, test_y)\n",
      "  scores.append(score)\n",
      "\n",
      "score = sum(scores) / len(scores)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 92
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "score"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 93,
       "text": [
        "0.85692181629182984"
       ]
      }
     ],
     "prompt_number": 93
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "examples = [data_frame2[\"text\"][2]]\n",
      "example_counts = count_vectorizer.transform(examples)\n",
      "predictions = classifier.predict(example_counts)\n",
      "str(predictions)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 85,
       "text": [
        "\"['YES']\""
       ]
      }
     ],
     "prompt_number": 85
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "correct = 0\n",
      "for i in range(5):\n",
      "    print type(data_frame2[\"text\"][i])\n",
      "    y = pipeline.predict([data_frame2[\"text\"][i]])\n",
      "    print y\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<type 'unicode'>\n",
        "['NO']\n",
        "<type 'unicode'>\n",
        "['NO']\n",
        "<type 'unicode'>\n",
        "['YES']\n",
        "<type 'unicode'>\n",
        "['NO']\n",
        "<type 'unicode'>\n",
        "['NO']\n"
       ]
      }
     ],
     "prompt_number": 97
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(df1_tst)\n",
      "correct"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5000\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 25,
       "text": [
        "0"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df1_tr[\"ISDIAGPOSSIBLE\"].value_counts()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "NO                5688\n",
        "YES               4111\n",
        "CompleteCare       184\n",
        "Safety Capture      17\n",
        "dtype: int64"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df1_tst[\"ISDIAGPOSSIBLE\"].value_counts()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 22,
       "text": [
        "NO                2811\n",
        "YES               2047\n",
        "CompleteCare       129\n",
        "Safety Capture      13\n",
        "dtype: int64"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(df1_tr)\n",
      "print len(df1_tst)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "10000\n",
        "5000\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df1_tr['TUPLE'] = zip(df1_tr['free_text'], df1_tr[column_tr])\n",
      "tupleList = df1_tr[\"TUPLE\"].tolist()\n",
      "\n",
      "df1_tst['TUPLE'] = zip(df1_tst['free_text'], df1_tst[column_tst])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in range(len(df1_tr)):\n",
      "    lowers = str(df1_tr[\"free_text\"][i]).lower()\n",
      "    replace_punctuation = string.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
      "    token_dict[i] = lowers.translate(replace_punctuation)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "type(token_dict)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 28,
       "text": [
        "dict"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "newdict = {}\n",
      "for i in range(len(token_dict)-1):\n",
      "    token_dict[i] = token_dict[i].decode('utf-8','ignore')\n",
      "    newdict[i] = unaccent(token_dict[i])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words='english')\n",
      "tfs = tfidf.fit_transform(newdict.values())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "feature_names = tfidf.get_feature_names()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "all_words = []\n",
      "\n",
      "for i in range(len(df1_tr)):\n",
      "    df1_tr[\"free_text\"][i] = df1_tr[\"free_text\"][i].translate(replace_punctuation)\n",
      "    df1_tr[\"free_text\"][i] = df1_tr[\"free_text\"][i].decode('utf-8','ignore')\n",
      "    df1_tr[\"free_text\"][i] = unaccent(df1_tr[\"free_text\"][i])\n",
      "    response = tfidf.transform([df1_tr[\"free_text\"][i]])\n",
      "    s = {}\n",
      "    for j in range(len(response.nonzero()[1])):\n",
      "        k = response.nonzero()[1][j]\n",
      "        s[k] = response[0,k]\n",
      "    sorted_s = sorted(s.iteritems(), key=operator.itemgetter(1))\n",
      "    if len(sorted_s) > 20:\n",
      "        top20 = sorted_s[:20]\n",
      "    else:\n",
      "        top20 = sorted_s\n",
      "    for x,y in top20:\n",
      "        all_words.append(feature_names[x])\n",
      "        \n",
      "all_words = set(all_words)  \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_words = nltk.FreqDist(all_words)\n",
      "  \n",
      "len(all_words.keys())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 33,
       "text": [
        "3488"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "word_features = []\n",
      "for i in all_words.keys():\n",
      "    if len(i) > 2:\n",
      "        word_features.append(i)\n",
      "        \n",
      "#word_features"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_set = [(document_features(d), c) for (d,c) in df1_tr['TUPLE']]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_set = [(document_features(d), c) for (d,c) in df1_tst['TUPLE']]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
      "classifier"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "accu = nltk.classify.accuracy(classifier, test_set)\n",
      "print accu"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#classifier.show_most_informative_features(150)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 169
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for x,y in test_set[0:2]:\n",
      "    p = classifier.prob_classify(x)\n",
      "    for sample in p.samples():\n",
      "        print \"{0}: {1}\".format(sample, p.prob(sample))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "YES: 0.770029001241\n",
        "NO: 0.229970998759\n",
        "YES: 0.999761543854\n",
        "NO: 0.000238456146454\n"
       ]
      }
     ],
     "prompt_number": 170
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "correct1 = 0\n",
      "cases_classified1 = 0\n",
      "for i in range(2):\n",
      "    p = classifier.prob_classify(test_set[i][0])\n",
      "    prob_NO = p.prob(sample)\n",
      "    print prob_NO\n",
      "    print test_set[i][1]\n",
      "    if prob_NO < 0.01:\n",
      "        cases_classified1 = cases_classified1 + 1  \n",
      "        if test_set[i][1] == \"YES\":\n",
      "            correct1 = correct1 + 1\n",
      "    elif  prob_NO > 0.99:\n",
      "        cases_classified1 = cases_classified1 + 1 \n",
      "        if test_set[i][1] == \"NO\":\n",
      "            correct1 = correct1 + 1\n",
      "        \n",
      "Accuracy1 = correct1/cases_classified1\n",
      "print Accuracy1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.229970998759\n",
        "YES\n",
        "0.000238456146454\n",
        "YES\n",
        "1.0\n"
       ]
      }
     ],
     "prompt_number": 171
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "correct = 0\n",
      "cases_classified = 0\n",
      "for i in range(len(test_set)):\n",
      "    p = classifier.prob_classify(test_set[i][0])\n",
      "    prob_NO = p.prob(sample)\n",
      "    if prob_NO < 0.01:\n",
      "        cases_classified = cases_classified + 1  \n",
      "        if test_set[i][1] == \"YES\":\n",
      "            correct = correct + 1\n",
      "    elif  prob_NO > 0.99:\n",
      "        cases_classified = cases_classified + 1 \n",
      "        if test_set[i][1] == \"NO\":\n",
      "            correct = correct + 1\n",
      "        \n",
      "Accuracy = correct/cases_classified"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 172
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "refsets = collections.defaultdict(set)\n",
      "testsets = collections.defaultdict(set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 173
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i, (x,y) in enumerate(test_set):\n",
      "    refsets[y].add(i)\n",
      "    observed = classifier.classify(x)\n",
      "    testsets[observed].add(i)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 174
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"COLUMN                              :\",(column_tr)\n",
      "print \"COMMODITY                           :\",(commodity)\n",
      "print \"Length of test cases                :\",len(df1_tst)\n",
      "print \"Accuracy on total test cases is     :\",(accu)\n",
      "print \"cases_classified with prob. >99% is :\",(cases_classified)\n",
      "print \"Accuracy with >99% prob. is         :\",(Accuracy)\n",
      "\n",
      "print df1_tr[\"ISDIAGPOSSIBLE\"].value_counts()\n",
      "print df1_tst[\"ISDIAGPOSSIBLE\"].value_counts()\n",
      "\n",
      "print 'YES precision                       :', nltk.metrics.precision(refsets['YES'], testsets['YES'])\n",
      "print 'YES recall                          :', nltk.metrics.recall(refsets['YES'], testsets['YES'])\n",
      "print 'YES F-measure                       :', nltk.metrics.f_measure(refsets['YES'], testsets['YES'])\n",
      "print 'NO precision                        :', nltk.metrics.precision(refsets['NO'], testsets['NO'])\n",
      "print 'NO recall                           :', nltk.metrics.recall(refsets['NO'], testsets['NO'])\n",
      "print 'NO F-measure                        :', nltk.metrics.f_measure(refsets['NO'], testsets['NO'])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "COLUMN                              : PROCESS POLICIES FOLLOWED (YES/NO)\n",
        "COMMODITY                           : Hard Drive\n",
        "Length of test cases                : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5000\n",
        "Accuracy on total test cases is     : 0.805952141564\n",
        "cases_classified with prob. >99% is : 3123\n",
        "Accuracy with >99% prob. is         : 0.878962536023\n",
        "Yes                  7258\n",
        "No                   2682\n",
        "No Fusion id only      39\n",
        "No Kana id only        21\n",
        "dtype: int64"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Yes                  4508\n",
        "No                    465\n",
        "No Kana id only        24\n",
        "No Fusion id only       3\n",
        "dtype: int64"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "YES precision                       : "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.921685313021\n",
        "YES recall                          : 0.858917480035\n",
        "YES F-measure                       : 0.889195085544\n",
        "NO precision                        : 0.176165803109\n",
        "NO recall                           : 0.29247311828\n",
        "NO F-measure                        : 0.219886822959\n"
       ]
      }
     ],
     "prompt_number": 175
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}